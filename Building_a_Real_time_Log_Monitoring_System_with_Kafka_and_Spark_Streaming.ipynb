{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdMsNx7/YWEcRY/j+AoROw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdev.com/building-a-real-time-log-monitoring-system-with-kafka-and-spark-streaming-51b75ac8db04)"
      ],
      "metadata": {
        "id": "eW3SRfVrWAky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Building a Kafka Producer"
      ],
      "metadata": {
        "id": "_icMsvTTV_OT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5ffsidVjV6Nd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from confluent_kafka import Producer\n",
        "import json\n",
        "\n",
        "# Setup configurations\n",
        "data_dir = \"/path/to/log/files\"  # directory path containing the log files\n",
        "kafka_broker = \"localhost:9092\"  # Kafka broker\n",
        "kafka_topic = \"log-topic\"  # Kafka topic\n",
        "\n",
        "# Create a producer to send data to Kafka\n",
        "producer = Producer({\n",
        "    'bootstrap.servers': kafka_broker,\n",
        "    'queue.buffering.max.messages': 10000000,  # Set the desired queue size\n",
        "    'compression.type': 'zstd'  # Or 'snappy', 'lz4', 'zstd'\n",
        "})\n",
        "\n",
        "def process_file(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            # Create a message to be sent to Kafka\n",
        "            message = {\n",
        "                'log_line': line.strip()  # Remove any trailing newline characters\n",
        "            }\n",
        "\n",
        "            # Serialize the message to JSON\n",
        "            message_json = json.dumps(message)\n",
        "\n",
        "            # Send the message to Kafka\n",
        "            producer.produce(topic=kafka_topic, value=message_json)\n",
        "\n",
        "        # Close the producer\n",
        "        producer.flush()\n",
        "\n",
        "# Get the list of files already processed\n",
        "processed_files = set()\n",
        "\n",
        "while True:\n",
        "    # List all log files in the directory\n",
        "    files = [f for f in os.listdir(data_dir) if f.endswith('.log')]\n",
        "\n",
        "    # Process any new files\n",
        "    for file in files:\n",
        "        if file not in processed_files:\n",
        "            process_file(os.path.join(data_dir, file))\n",
        "            processed_files.add(file)\n",
        "\n",
        "    # Wait for a while before checking the directory again\n",
        "    time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Building the Spark Streaming Application with Email Alerts"
      ],
      "metadata": {
        "id": "Esmd9N8nWDD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType, StringType\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Log Monitor\").getOrCreate()\n",
        "\n",
        "# Define the UDF to search for 'ERROR' keyword\n",
        "@udf(returnType=BooleanType())\n",
        "def contains_error(line):\n",
        "    line_json = json.loads(line)\n",
        "    return 'ERROR' in line_json['log_line']\n",
        "\n",
        "# Send email alert\n",
        "@udf(returnType=StringType())\n",
        "def send_email_alert(line):\n",
        "    error_line = json.loads(line)['log_line']\n",
        "\n",
        "    # Email content\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = 'your-email@gmail.com'\n",
        "    msg['To'] = 'receiver-email@gmail.com'\n",
        "    msg['Subject'] = 'Log Monitor Alert'\n",
        "    message = f'ERROR found: {error_line}'\n",
        "    msg.attach(MIMEText(message))\n",
        "\n",
        "    # Send the email\n",
        "    mailserver = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "    mailserver.ehlo()\n",
        "    mailserver.starttls()\n",
        "    mailserver.login('your-email@gmail.com', 'your-password')\n",
        "    mailserver.sendmail('your-email@gmail.com', 'receiver-email@gmail.com', msg.as_string())\n",
        "    mailserver.quit()\n",
        "\n",
        "    return 'Email Alert sent.'\n",
        "\n",
        "# Subscribe to Kafka topic\n",
        "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"log-topic\").load()\n",
        "\n",
        "# Filter the lines that contain 'ERROR'\n",
        "errorLines = df.filter(contains_error(col(\"value\")))\n",
        "\n",
        "# Send email alerts\n",
        "alert = errorLines.withColumn(\"Alert\", send_email_alert(col(\"value\")))\n",
        "\n",
        "# Write out the 'ERROR' lines and Alert status to console\n",
        "query = alert.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "X4vIXOoKV-H4"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}