{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaMlogGDKWKtsXVnXRy/XT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://levelup.gitconnected.com/kafka-in-machine-learning-for-real-time-predictions-45a4adf4620b)"
      ],
      "metadata": {
        "id": "l7QkBBz6q_y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9USVgqZaq10R"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from marshmallow_dataclass import class_schema\n",
        "import logging\n",
        "import sys\n",
        "from typing import List\n",
        "import yaml\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class SplittingParams:\n",
        "    val_size: float = field(default=0.2)\n",
        "    random_state: int = field(default=42)\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class FeatureParams:\n",
        "    categorical_features: List[str]\n",
        "    numerical_features: List[str]\n",
        "    target_col: str = field(default=\"target\")\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class TrainingParams:\n",
        "    model_type: str = field(default=\"RandomForestClassifier\")\n",
        "    random_state: int = field(default=42)\n",
        "    # RF params\n",
        "    max_depth: int = field(default=5)\n",
        "    # LR params\n",
        "    solver: str = field(default=\"lbfgs\")\n",
        "    C: float = field(default=1.0)\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class KafkaBrokerParams:\n",
        "    bootstrap_servers: str\n",
        "    security_protocol: str\n",
        "    sasl_mechanisms: str\n",
        "    sasl_username: str\n",
        "    sasl_password: str\n",
        "    train_topic: str\n",
        "    predict_topic: str\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class KafkaConsumerParams:\n",
        "    group_id: str\n",
        "    auto_offset_reset: str\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class TrainingPipelineParams:\n",
        "    output_data_featurized_path: str\n",
        "    output_data_target_path: str\n",
        "    output_data_train_path: str\n",
        "    output_data_test_path: str\n",
        "    output_target_train_path: str\n",
        "    output_target_test_path: str\n",
        "    output_model_path: str\n",
        "    output_transformer_path: str\n",
        "    metric_path: str\n",
        "    splitting_params: SplittingParams\n",
        "    feature_params: FeatureParams\n",
        "    train_params: TrainingParams\n",
        "    input_data_path: str = field(default=\"data/wines_SPA.csv\")\n",
        "\n",
        "\n",
        "@dataclass()\n",
        "class KafkaParams:\n",
        "    kafka_broker: KafkaBrokerParams\n",
        "    kafka_consumer: KafkaConsumerParams\n",
        "\n",
        "\n",
        "def read_training_pipeline_params(path: str) -> TrainingPipelineParams:\n",
        "    with open(path, \"r\") as input_stream:\n",
        "        config_dict = yaml.safe_load(input_stream)\n",
        "        schema = TrainingPipelineParamsSchema().load(config_dict)\n",
        "        logger.info(f\"Check schema: {schema}\")\n",
        "        return schema\n",
        "\n",
        "\n",
        "def read_kafka_params(path: str) -> KafkaParams:\n",
        "    with open(path, \"r\") as input_stream:\n",
        "        config_dict = yaml.safe_load(input_stream)\n",
        "        schema = KafkaParamsSchema().load(config_dict)\n",
        "        logger.info(f\"Check schema: {schema}\")\n",
        "        return schema\n",
        "\n",
        "\n",
        "TrainingPipelineParamsSchema = class_schema(TrainingPipelineParams)\n",
        "KafkaParamsSchema = class_schema(KafkaParams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "kafka_config.yaml\n",
        "kafka_broker:\n",
        "  bootstrap_servers: <server-endpoint>\n",
        "  security_protocol: \"SASL_SSL\"\n",
        "  sasl_mechanisms: \"PLAIN\"\n",
        "  sasl_username: <API-Key>\n",
        "  sasl_password: <Secret-Key>\n",
        "  train_topic: \"train\"\n",
        "  predict_topic: \"app_messages\"\n",
        "\n",
        "kafka_consumer:\n",
        "  group_id: \"mygroup\"\n",
        "  auto_offset_reset: \"earliest\"\n",
        "```"
      ],
      "metadata": {
        "id": "WXbbNgfwrItY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import logging\n",
        "from boto3 import client\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "CONFIG_PATH = \"configs/pred_config.yaml\"\n",
        "s3 = client(\"s3\")\n",
        "\n",
        "\n",
        "def download(\n",
        "    path=\"../models/model.pkl\", bucket_name=\"wines-models\",\n",
        "):\n",
        "    file_name = path.split(\"/\")[-1]\n",
        "    s3.download_file(bucket_name, file_name, path)\n",
        "\n",
        "\n",
        "def upload(\n",
        "    path=\"../models/model.pkl\", bucket_name=\"wines-models\",\n",
        "):\n",
        "    file_name = path.split(\"/\")[-1]\n",
        "    s3.upload_file(path, bucket_name, file_name)"
      ],
      "metadata": {
        "id": "fUcyGO9ItM9z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import datetime\n",
        "import sys\n",
        "import logging\n",
        "from typing import Union\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from confluent_kafka import Producer\n",
        "\n",
        "from featurization import read_training_pipeline_params, read_kafka_params\n",
        "from train import train_model, serialize_model\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "SklearnRegressor = Union[RandomForestRegressor, LinearRegression]\n",
        "\n",
        "\n",
        "def publish_train(producer: Producer, model_id: str, topic: str):\n",
        "    message_id = str(uuid.uuid4())\n",
        "    message_json = json.dumps({\"request_id\": message_id, \"model_id\": model_id}).encode(\n",
        "        \"utf-8\"\n",
        "    )\n",
        "    producer.produce(topic, key=message_id, value=message_json)\n",
        "    producer.flush()\n",
        "    logger.info(\"messages_json \\n {}\".format(message_json))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config_path = \"configs/train_config.yaml\"\n",
        "    kafka_config_path = \"configs/kafka_config.yaml\"\n",
        "\n",
        "    training_pipeline_params = read_training_pipeline_params(config_path)\n",
        "    kafka_params = read_kafka_params(kafka_config_path)\n",
        "\n",
        "    train_features = pd.read_csv(training_pipeline_params.output_data_train_path)\n",
        "    train_target = pd.read_csv(training_pipeline_params.output_target_train_path)\n",
        "\n",
        "    model = train_model(\n",
        "        train_features, train_target[\"price\"], training_pipeline_params.train_params\n",
        "    )\n",
        "\n",
        "    producer = Producer(\n",
        "        {\n",
        "            \"bootstrap.servers\": kafka_params.kafka_broker.bootstrap_servers,\n",
        "            \"security.protocol\": kafka_params.kafka_broker.security_protocol,\n",
        "            \"sasl.mechanisms\": kafka_params.kafka_broker.sasl_mechanisms,\n",
        "            \"sasl.username\": kafka_params.kafka_broker.sasl_username,\n",
        "            \"sasl.password\": kafka_params.kafka_broker.sasl_password,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    current_datetime = datetime.datetime.now()\n",
        "    year, month, day = map(\n",
        "        lambda x: \"{:02d}\".format(x),\n",
        "        (current_datetime.year, current_datetime.month, current_datetime.day),\n",
        "    )\n",
        "    day_month_year = \"{}-{}-{}\".format(year, month, day)\n",
        "    model_id = \"models/\" + day_month_year + \".pkl\"\n",
        "\n",
        "    serialize_model(model, model_id)\n",
        "    publish_train(producer, model_id, kafka_params.kafka_broker.train_topic)"
      ],
      "metadata": {
        "id": "iX-U9zMttNde"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import uuid\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "from confluent_kafka import Consumer, Producer\n",
        "\n",
        "from featurization import (\n",
        "    read_training_pipeline_params,\n",
        "    read_kafka_params,\n",
        "    TrainingPipelineParams,\n",
        "    KafkaParams,\n",
        ")\n",
        "from src.artefacts_s3 import download\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "\n",
        "MODEL_PATH = \"models/2022-08-19.pkl\"\n",
        "\n",
        "\n",
        "def publish_prediction(producer: Producer, pred: np.ndarray, topic: str):\n",
        "    message_id = str(uuid.uuid4())\n",
        "    message_json = json.dumps({\"request_id\": message_id, \"pred\": pred[0]}).encode(\n",
        "        \"utf-8\"\n",
        "    )\n",
        "    producer.produce(topic, key=message_id, value=message_json)\n",
        "    producer.flush()\n",
        "    logger.info(\"messages_json \\n {}\".format(str(message_json)))\n",
        "\n",
        "\n",
        "def load_model(params: TrainingPipelineParams, path: str):\n",
        "    download(path)\n",
        "    logger.info(\"model loaded on path \\n {}\".format(str(path)))\n",
        "    return joblib.load(params.output_model_path)\n",
        "\n",
        "\n",
        "def predict(train_params: TrainingPipelineParams, kafka_params: KafkaParams):\n",
        "    producer = Producer(\n",
        "        {\n",
        "            \"bootstrap.servers\": kafka_params.kafka_broker.bootstrap_servers,\n",
        "            \"security.protocol\": kafka_params.kafka_broker.security_protocol,\n",
        "            \"sasl.mechanisms\": kafka_params.kafka_broker.sasl_mechanisms,\n",
        "            \"sasl.username\": kafka_params.kafka_broker.sasl_username,\n",
        "            \"sasl.password\": kafka_params.kafka_broker.sasl_password,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    test_features = pd.read_csv(train_params.output_data_test_path)\n",
        "    model = load_model(params=train_params, path=MODEL_PATH)\n",
        "\n",
        "    for index, test_row in test_features.iterrows():\n",
        "        pred = model.predict(pd.DataFrame(test_row).T)\n",
        "        publish_prediction(producer, pred, kafka_params.kafka_broker.predict_topic)\n",
        "\n",
        "\n",
        "def read_prediction(kafka_params: KafkaParams):\n",
        "    consumer = Consumer(\n",
        "        {\n",
        "            \"bootstrap.servers\": kafka_params.kafka_broker.bootstrap_servers,\n",
        "            \"group.id\": kafka_params.kafka_consumer.group_id,\n",
        "            \"auto.offset.reset\": kafka_params.kafka_consumer.auto_offset_reset,\n",
        "            \"security.protocol\": kafka_params.kafka_broker.security_protocol,\n",
        "            \"sasl.mechanisms\": kafka_params.kafka_broker.sasl_mechanisms,\n",
        "            \"sasl.username\": kafka_params.kafka_broker.sasl_username,\n",
        "            \"sasl.password\": kafka_params.kafka_broker.sasl_password,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    consumer.subscribe([kafka_params.kafka_broker.predict_topic])\n",
        "    msg = consumer.poll(timeout=1.0)\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if msg is None:\n",
        "                logger.info(\"None, waiting...\")\n",
        "            elif msg.error():\n",
        "                logger.info(\"ERROR: %s\".format(msg.error()))\n",
        "            else:\n",
        "                logger.info(\n",
        "                    \"\\033[1;32;40m ** CONSUMER: message for request_id {}, pred {}\".format(\n",
        "                        msg.key(), msg.value()\n",
        "                    )\n",
        "                )\n",
        "                break\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        # Leave group and commit final offsets\n",
        "        consumer.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config_path = \"configs/train_config.yaml\"\n",
        "    kafka_config_path = \"configs/kafka_config.yaml\"\n",
        "\n",
        "    training_pipeline_params = read_training_pipeline_params(config_path)\n",
        "    kafka_pipeline_params = read_kafka_params(kafka_config_path)\n",
        "\n",
        "    predict(training_pipeline_params, kafka_pipeline_params)\n",
        "    read_prediction(kafka_pipeline_params)\n",
        "\n",
        "consumer.subscribe([kafka_params.kafka_broker.predict_topic])\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        msg = consumer.poll(timeout=1.0)\n",
        "        if msg is None:\n",
        "            logger.info(\"None, waiting...\")\n",
        "        elif msg.error():\n",
        "            logger.info(\"ERROR: %s\".format(msg.error()))\n",
        "        else:\n",
        "            logger.info(\n",
        "                \"\\033[1;32;40m ** CONSUMER: message for request_id {}, pred {}\".format(\n",
        "                    msg.key(), msg.value()\n",
        "                )\n",
        "            )\n",
        "            break\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    # Leave group and commit final offsets\n",
        "    consumer.close()"
      ],
      "metadata": {
        "id": "er6C8kTbtUTw"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}