{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5iArc26I/Lozu4IQiSYI3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@evertongomede/batch-vs-streaming-data-ingestion-choosing-the-right-approach-for-efficient-data-processing-8fa492299dd4)"
      ],
      "metadata": {
        "id": "S15w0I2u-0mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Ingestion (Using CSV File)"
      ],
      "metadata": {
        "id": "XYcfFS_W_KMk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n4tXQ1tU-tRg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to ingest data in batch from a CSV file\n",
        "def batch_data_ingestion(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Process the data as needed\n",
        "    for index, row in df.iterrows():\n",
        "        # Your data processing logic here\n",
        "        print(row)\n",
        "    print(\"Batch data ingestion completed.\")\n",
        "\n",
        "# Replace 'path_to_csv_file.csv' with the actual path of your CSV file\n",
        "batch_data_ingestion(\"path_to_csv_file.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Ingestion (Using Apache Kafka)"
      ],
      "metadata": {
        "id": "6gqTHu9J_Ows"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "# Function to send data in streaming to a Kafka topic\n",
        "def send_data_to_kafka_topic(producer, topic, data):\n",
        "    producer.send(topic, value=data.encode('utf-8'))\n",
        "    producer.flush()\n",
        "\n",
        "# Function to ingest data from a Kafka topic in streaming\n",
        "def streaming_data_ingestion(consumer, topic):\n",
        "    for message in consumer:\n",
        "        data = message.value.decode('utf-8')\n",
        "        # Your data processing logic here\n",
        "        print(data)\n",
        "\n",
        "# Replace 'kafka_broker_address:port' with the actual address and port of your Kafka broker\n",
        "kafka_broker = 'kafka_broker_address:port'\n",
        "topic_name = 'your_topic_name'\n",
        "\n",
        "# Create Kafka producer\n",
        "producer = KafkaProducer(bootstrap_servers=kafka_broker)\n",
        "\n",
        "# Replace 'streaming_data' with your actual streaming data (e.g., sensor data, logs, etc.)\n",
        "streaming_data = \"streaming_data_example\"\n",
        "\n",
        "# Send data to the Kafka topic\n",
        "send_data_to_kafka_topic(producer, topic_name, streaming_data)\n",
        "\n",
        "# Create Kafka consumer\n",
        "consumer = KafkaConsumer(topic_name, bootstrap_servers=kafka_broker)\n",
        "\n",
        "# Ingest data from the Kafka topic in streaming\n",
        "streaming_data_ingestion(consumer, topic_name)"
      ],
      "metadata": {
        "id": "eMDOnXRC_MB2"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}